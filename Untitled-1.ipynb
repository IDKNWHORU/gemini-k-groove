{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: opencv-python in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (4.10.0.84)\n",
      "Requirement already satisfied: numpy>=1.21.2 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from opencv-python) (1.26.4)\n",
      "Requirement already satisfied: mediapipe in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (0.10.14)\n",
      "Requirement already satisfied: absl-py in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from mediapipe) (2.1.0)\n",
      "Requirement already satisfied: attrs>=19.1.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from mediapipe) (23.2.0)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from mediapipe) (24.3.25)\n",
      "Requirement already satisfied: jax in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from mediapipe) (0.4.31)\n",
      "Requirement already satisfied: jaxlib in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from mediapipe) (0.4.31)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from mediapipe) (3.9.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from mediapipe) (1.26.4)\n",
      "Requirement already satisfied: opencv-contrib-python in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from mediapipe) (4.10.0.84)\n",
      "Requirement already satisfied: protobuf<5,>=4.25.3 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from mediapipe) (4.25.4)\n",
      "Requirement already satisfied: sounddevice>=0.4.4 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from mediapipe) (0.4.7)\n",
      "Requirement already satisfied: CFFI>=1.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sounddevice>=0.4.4->mediapipe) (1.16.0)\n",
      "Requirement already satisfied: ml-dtypes>=0.2.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jax->mediapipe) (0.4.0)\n",
      "Requirement already satisfied: opt-einsum in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jax->mediapipe) (3.3.0)\n",
      "Requirement already satisfied: scipy>=1.10 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jax->mediapipe) (1.14.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib->mediapipe) (1.2.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib->mediapipe) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib->mediapipe) (4.53.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib->mediapipe) (1.4.5)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from matplotlib->mediapipe) (24.1)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib->mediapipe) (10.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib->mediapipe) (3.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from matplotlib->mediapipe) (2.9.0.post0)\n",
      "Requirement already satisfied: pycparser in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from CFFI>=1.0->sounddevice>=0.4.4->mediapipe) (2.22)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from python-dateutil>=2.7->matplotlib->mediapipe) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install opencv-python\n",
    "!pip install mediapipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "\n",
    "BaseOptions = mp.tasks.BaseOptions\n",
    "PoseLandmarker = mp.tasks.vision.PoseLandmarker\n",
    "PoseLandmarkerOptions = mp.tasks.vision.PoseLandmarkerOptions\n",
    "VisionRunningMode = mp.tasks.vision.RunningMode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\USER\\\\workspace\\\\idol\\\\pose_landmarker_full.task'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.join(os.getcwd(), 'pose_landmarker_full.task')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing expert video...\n",
      "Extracted 569 poses from expert video\n",
      "Saved expert poses to expert_poses.json\n"
     ]
    }
   ],
   "source": [
    "# Define the create_pose_landmarker function\n",
    "def create_pose_landmarker():\n",
    "    model_path = os.path.join(os.getcwd(), 'pose_landmarker_full.task')\n",
    "    options = mp.tasks.vision.PoseLandmarkerOptions(\n",
    "        base_options=mp.tasks.BaseOptions(model_asset_path=model_path),\n",
    "        running_mode=mp.tasks.vision.RunningMode.VIDEO,\n",
    "        num_poses=1,\n",
    "        min_pose_detection_confidence=0.5,\n",
    "        min_pose_presence_confidence=0.5,\n",
    "        min_tracking_confidence=0.5,\n",
    "        output_segmentation_masks=False\n",
    "    )\n",
    "    return mp.tasks.vision.PoseLandmarker.create_from_options(options)\n",
    "\n",
    "# Define the process_video function\n",
    "def process_video(video_path, landmarker, duration_s=3):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    total_frames = int(duration_s * fps)\n",
    "    poses = []\n",
    "    frame_count = 0\n",
    "    \n",
    "    while cap.isOpened() and frame_count < total_frames:\n",
    "        success, image = cap.read()\n",
    "        if not success:\n",
    "            break\n",
    "        \n",
    "        mp_image = mp.Image(image_format=mp.ImageFormat.SRGB, data=cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "        \n",
    "        # Use frame count for timestamp instead of CAP_PROP_POS_MSEC\n",
    "        frame_timestamp_ms = int(frame_count * (1000 / fps))\n",
    "        \n",
    "        pose_landmarker_result = landmarker.detect_for_video(mp_image, frame_timestamp_ms)\n",
    "        \n",
    "        if pose_landmarker_result.pose_landmarks:\n",
    "            poses.append(pose_landmarker_result.pose_landmarks[0])\n",
    "        \n",
    "        frame_count += 1\n",
    "    \n",
    "    cap.release()\n",
    "    return poses\n",
    "\n",
    "# Function to convert pose landmarks to a list of dictionaries\n",
    "def poses_to_json_data(poses):\n",
    "    json_data = []\n",
    "    for pose in poses:\n",
    "        landmarks = []\n",
    "        for landmark in pose:\n",
    "            landmarks.append({\n",
    "                \"x\": landmark.x,\n",
    "                \"y\": landmark.y,\n",
    "                \"z\": landmark.z\n",
    "            })\n",
    "        json_data.append(landmarks)\n",
    "    return json_data\n",
    "\n",
    "# Main execution code\n",
    "expert_video = \"./expert_dance.mp4\"\n",
    "user_video = \"./user_dance.mp4\"\n",
    "\n",
    "with create_pose_landmarker() as landmarker:\n",
    "    print(\"Processing expert video...\")\n",
    "    expert_poses = process_video(expert_video, landmarker, duration_s=19)\n",
    "    print(f\"Extracted {len(expert_poses)} poses from expert video\")\n",
    "\n",
    "    # Convert expert_poses to JSON format\n",
    "    expert_poses_json = poses_to_json_data(expert_poses)\n",
    "    \n",
    "    # Save expert_poses to a JSON file\n",
    "    with open(\"expert_poses.json\", \"w\") as f:\n",
    "        json.dump({\"export_poses\": expert_poses_json}, f, indent=4)\n",
    "    print(\"Saved expert poses to expert_poses.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing user video...\n",
      "Extracted 630 poses from user video\n"
     ]
    }
   ],
   "source": [
    "with create_pose_landmarker() as landmarker:\n",
    "    print(\"Processing user video...\")\n",
    "    user_poses = process_video(user_video, landmarker, duration_s=21)\n",
    "    print(f\"Extracted {len(user_poses)} poses from user video\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_pose_data(expert_poses, user_poses):\n",
    "    # 포즈 데이터를 Gemini에 전송할 수 있는 형식으로 변환\n",
    "    formatted_data = {\n",
    "        \"expert_poses\": [\n",
    "            [{\"x\": lm.x, \"y\": lm.y, \"z\": lm.z} for lm in pose]\n",
    "            for pose in expert_poses[:150]  # 처음 150개 프레임만 사용\n",
    "        ],\n",
    "        \"user_poses\": [\n",
    "            [{\"x\": lm.x, \"y\": lm.y, \"z\": lm.z} for lm in pose]\n",
    "            for pose in user_poses[:150]  # 처음 150개 프레임만 사용\n",
    "        ]\n",
    "    }\n",
    "    return json.dumps(formatted_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "formatted_data = format_pose_data(expert_poses, user_poses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q -U google-generativeai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare the following expert and user pose data\n",
    "\n",
    "## Expert and User video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<div style=\"display: flex;\">\n",
       "  <div style=\"flex: 1;\"><video src=\"./expert_dance.mp4\" controls  >\n",
       "      Your browser does not support the <code>video</code> element.\n",
       "    </video></div>\n",
       "  <div style=\"flex: 1;\"><video src=\"./user_dance.mp4\" controls  >\n",
       "      Your browser does not support the <code>video</code> element.\n",
       "    </video></div>\n",
       "</div>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML, Video\n",
    "\n",
    "video1 = Video(\"./expert_dance.mp4\")\n",
    "video2 = Video(\"./user_dance.mp4\")\n",
    "\n",
    "html_code = \"\"\"\n",
    "<div style=\"display: flex;\">\n",
    "  <div style=\"flex: 1;\">{video1}</div>\n",
    "  <div style=\"flex: 1;\">{video2}</div>\n",
    "</div>\n",
    "\"\"\".format(video1=video1._repr_html_(), video2=video2._repr_html_())\n",
    "\n",
    "display(HTML(html_code))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expert and User pose video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<div style=\"display: flex;\">\n",
       "  <div style=\"flex: 1;\"><video src=\"./expert_pose.mp4\" controls  >\n",
       "      Your browser does not support the <code>video</code> element.\n",
       "    </video></div>\n",
       "  <div style=\"flex: 1;\"><video src=\"./user_pose.mp4\" controls  >\n",
       "      Your browser does not support the <code>video</code> element.\n",
       "    </video></div>\n",
       "</div>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML, Video\n",
    "\n",
    "p_video1 = Video(\"./expert_pose.mp4\")\n",
    "p_video2 = Video(\"./user_pose.mp4\")\n",
    "\n",
    "html_code = \"\"\"\n",
    "<div style=\"display: flex;\">\n",
    "  <div style=\"flex: 1;\">{p_video1}</div>\n",
    "  <div style=\"flex: 1;\">{p_video2}</div>\n",
    "</div>\n",
    "\"\"\".format(p_video1=p_video1._repr_html_(), p_video2=p_video2._repr_html_())\n",
    "\n",
    "display(HTML(html_code))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0~150 Frame Poses Compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Feedback: Pose data anlysis(0~5s)\n",
      "**1. feedback: **  The user's left arm is not straight during the pose.\n",
      "** enhancement: **  Focus on keeping the left arm straight in all frames.\n",
      "\n",
      "**2. feedback: **  The user's left hand is not in the correct position and seems to be too far from the body.\n",
      "** enhancement: **  Keep the left hand closer to the body and practice the correct hand placement.\n",
      "\n",
      "**3. feedback: **  The user's right hand is not in the correct position and seems to be too far from the body.\n",
      "** enhancement: **  Keep the right hand closer to the body and practice the correct hand placement.\n",
      "\n",
      "**4. feedback: **  The user's right leg is not straight during the pose.\n",
      "** enhancement: **  Focus on keeping the right leg straight in all frames.\n",
      "\n",
      "**5. feedback: ** The user's left leg is not straight during the pose.\n",
      "** enhancement: **  Focus on keeping the left leg straight in all frames.\n",
      "\n",
      "**6. feedback: ** The user's hips are not in the correct position and seem to be too low.\n",
      "** enhancement: **  Practice raising the hips to the correct position.\n",
      "\n",
      "**7. feedback: ** The user's shoulders are not in the correct position and seem to be too low.\n",
      "** enhancement: **  Practice lifting the shoulders to the correct position.\n",
      "\n",
      "**8. feedback: ** The user's head is not in the correct position and seems to be too low.\n",
      "** enhancement: **  Practice lifting the head to the correct position.\n",
      "\n",
      "\n",
      "## Weekness Point: \n",
      "* Left arm is not straight.\n",
      "* Left hand is not in the correct position.\n",
      "* Right hand is not in the correct position.\n",
      "* Right leg is not straight.\n",
      "* Left leg is not straight.\n",
      "* Hips are too low.\n",
      "* Shoulders are too low.\n",
      "* Head is too low.\n",
      "\n",
      "## How to training: \n",
      "* Practice the pose in front of a mirror and focus on correcting each of the weakness points. \n",
      "* Use a video recording to analyze your form and identify any areas that need improvement.\n",
      "* Break down the pose into smaller parts and practice each part separately.\n",
      "* Ask a friend or instructor for feedback on your form.\n",
      "* Don't be afraid to ask for help from an instructor.\n"
     ]
    }
   ],
   "source": [
    "import google.generativeai as genai\n",
    "\n",
    "genai.configure(api_key=os.getenv('GEMINI_API_KEY'))\n",
    "\n",
    "model = genai.GenerativeModel('gemini-1.5-flash')\n",
    "\n",
    "system = { \"role\": \"system\", \"content\": f\"You are a dance instructor analyzing pose data. \\\n",
    "          Response Form: markdown \\\n",
    "          # Feedback: Pose data anlysis(0~5s)\\\n",
    "          \\\n",
    "          **1. feedback: ** \\\n",
    "          ** enhancement: ** \\\n",
    "          \\\n",
    "          **2. feedback: ** \\\n",
    "          ** enhancement: ** \\\n",
    "          \\\n",
    "          **3. feedback: ** \\\n",
    "          ** enhancement: ** \\\n",
    "          \\\n",
    "          **4. feedback: ** \\\n",
    "          ** enhancement: ** \\\n",
    "          ... \\\n",
    "          \\\n",
    "          ## Weekness Point: \\\n",
    "          ## How to training: \\\n",
    "          \"}\n",
    "\n",
    "user = { \"role\": \"user\", \"content\": f\"Compare the following expert and user pose data for approximately 5 second of 150 frames and provide feedback on the user's performance. Focus on major differences and suggest improvements. {formatted_data}\"}\n",
    "\n",
    "response = model.generate_content(f\"[{system}, {user}]\")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_pose_data2(expert_poses, user_poses):\n",
    "    # 포즈 데이터를 GPT에 전송할 수 있는 형식으로 변환\n",
    "    formatted_data = {\n",
    "        \"expert_poses\": [\n",
    "            [{\"x\": lm.x, \"y\": lm.y, \"z\": lm.z} for lm in pose]\n",
    "            for pose in expert_poses[150:300]  # 처음 90개 프레임만 사용\n",
    "        ],\n",
    "        \"user_poses\": [\n",
    "            [{\"x\": lm.x, \"y\": lm.y, \"z\": lm.z} for lm in pose]\n",
    "            for pose in user_poses[150:300]  # 처음 90개 프레임만 사용\n",
    "        ]\n",
    "    }\n",
    "    return json.dumps(formatted_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "formatted_data2 = format_pose_data2(expert_poses, user_poses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 151~300 Frame Poses Compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Feedback: Pose data anlysis(5~10s)\n",
      "**1. feedback: ** The user's left leg is not as extended as the expert's left leg. \n",
      "** enhancement: ** The user should focus on extending the left leg more to match the expert's pose.\n",
      "**2. feedback: ** The user's right leg is not fully extended during the final pose. \n",
      "** enhancement: ** The user needs to straighten out the right leg at the end of the move.\n",
      "**3. feedback: ** The user's right arm is not as high as the expert's right arm. \n",
      "** enhancement: ** The user should raise the right arm to match the expert's height. \n",
      "**4. feedback: ** User's right leg is not as straight and the knee is not pointed at the correct angle in the final pose. \n",
      "** enhancement: ** The user should straighten the knee to match the expert's leg.\n",
      "\n",
      "## Weekness Point: \n",
      "- The user is not fully extending their right leg and left leg.\n",
      "- The user is not fully extending their right arm. \n",
      "- The user's leg and knee angle is incorrect in the final pose. \n",
      "\n",
      "## How to training: \n",
      "- User should focus on fully extending the right leg, left leg, and right arm.\n",
      "- User should pay attention to the angle of the knee and the straightening of the right leg at the end of the move. \n",
      "- The user should use a mirror to help them visualize their posture and make adjustments as needed.  \n",
      "- The user can practice with a dance instructor or a friend who is familiar with the dance move to help them identify areas for improvement. \n",
      "- The user can break down the move into smaller parts and practice each part individually until they are able to perform it correctly. \n",
      "- The user can practice the move regularly to improve their muscle memory and coordination.\n"
     ]
    }
   ],
   "source": [
    "import google.generativeai as genai\n",
    "\n",
    "genai.configure(api_key=os.getenv('GEMINI_API_KEY'))\n",
    "\n",
    "model = genai.GenerativeModel('gemini-1.5-flash')\n",
    "\n",
    "system2 = { \"role\": \"system\", \"content\": f\"You are a dance instructor analyzing pose data. \\\n",
    "          Response Form: markdown \\\n",
    "          # Feedback: Pose data anlysis(5~10s)\\\n",
    "          \\\n",
    "          **1. feedback: ** \\\n",
    "          ** enhancement: ** \\\n",
    "            \\\n",
    "          **2. feedback: ** \\\n",
    "          ** enhancement: ** \\\n",
    "            \\\n",
    "          **3. feedback: ** \\\n",
    "          ** enhancement: ** \\\n",
    "            \\\n",
    "          **4. feedback: ** \\\n",
    "          ** enhancement: ** \\\n",
    "          ... \\\n",
    "            \\\n",
    "          ## Weekness Point: \\\n",
    "          ## How to training: \\\n",
    "          \"}\n",
    "\n",
    "user2 = { \"role\": \"user\", \"content\": f\"Compare the following expert and user pose data for approximately between 5 and 10 second of 150 frames and provide feedback on the user's performance. Focus on major differences and suggest improvements. {formatted_data2}\"}\n",
    "\n",
    "response2 = model.generate_content(f\"[{system2}, {user2}]\")\n",
    "\n",
    "print(response2.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 301~450 Frame Poses Compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Feedback: Pose data anlysis(10~15s)\n",
      "**1. feedback: ** The user is leaning too far forward in the upper body. This can lead to imbalances and reduce overall efficiency.\n",
      "** enhancement: ** Maintain a more upright posture, keeping the spine straight and the shoulders relaxed. \n",
      "\n",
      "**2. feedback: ** The user's left arm is not extending fully during the movement, which limits the range of motion.\n",
      "** enhancement: ** Focus on extending the left arm fully, reaching outward while keeping the shoulder engaged.\n",
      "\n",
      "**3. feedback: ** The user's right leg is not fully extending during the leg sweep, resulting in a less dynamic move.\n",
      "** enhancement: ** Actively push through the right heel to extend the leg fully and create a stronger sweep.\n",
      "\n",
      "**4. feedback: ** The user's right foot placement is inconsistent, sometimes too far back and sometimes too close to the body. \n",
      "** enhancement: ** Practice maintaining a consistent right foot position, keeping it close enough to the body for stability but not so close that it restricts movement.\n",
      "\n",
      "**5. feedback: ** The user's right arm is not moving with the same rhythm as the left arm. This disrupts the flow of the movement.\n",
      "** enhancement: **  Focus on coordinating the movement of both arms, ensuring that they move together seamlessly.\n",
      "\n",
      "**6. feedback: ** The user's transitions between movements are not smooth, resulting in a choppy execution. \n",
      "** enhancement: **  Practice smooth transitions between movements by engaging the core and flowing from one pose to the next.\n",
      "\n",
      "## Weekness Point: \n",
      "- Lack of core engagement and upper body stability\n",
      "- Inconsistent foot placement\n",
      "- Lack of coordination between arms\n",
      "\n",
      "## How to training: \n",
      "- **Core strength:** Focus on exercises like planks, crunches, and Russian twists to improve core strength and stability.\n",
      "- **Footwork drills:** Practice controlled movements with consistent right foot placement.\n",
      "- **Coordination exercises:**  Incorporate exercises that require simultaneous movement of both arms, like arm circles, overhead reaches, and arm swings.\n",
      "- **Slow motion practice:** Repeat the sequence in slow motion, focusing on precision and fluidity of movement.\n",
      "- **Mirror practice:** Use a mirror to visualize your posture and alignment and identify areas for improvement. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "def format_pose_data3(expert_poses, user_poses):\n",
    "    # 포즈 데이터를 GPT에 전송할 수 있는 형식으로 변환\n",
    "    formatted_data = {\n",
    "        \"expert_poses\": [\n",
    "            [{\"x\": lm.x, \"y\": lm.y, \"z\": lm.z} for lm in pose]\n",
    "            for pose in expert_poses[300:450]  # 처음 90개 프레임만 사용\n",
    "        ],\n",
    "        \"user_poses\": [\n",
    "            [{\"x\": lm.x, \"y\": lm.y, \"z\": lm.z} for lm in pose]\n",
    "            for pose in user_poses[300:450]  # 처음 90개 프레임만 사용\n",
    "        ]\n",
    "    }\n",
    "    return json.dumps(formatted_data)\n",
    "\n",
    "formatted_data3 = format_pose_data3(expert_poses, user_poses)\n",
    "\n",
    "import google.generativeai as genai\n",
    "\n",
    "genai.configure(api_key=os.getenv('GEMINI_API_KEY'))\n",
    "\n",
    "model = genai.GenerativeModel('gemini-1.5-flash')\n",
    "\n",
    "system3 = { \"role\": \"system\", \"content\": f\"You are a dance instructor analyzing pose data. \\\n",
    "            \\\n",
    "          Response Form: markdown \\\n",
    "          # Feedback: Pose data anlysis(10~15s)\\\n",
    "          \\\n",
    "          **1. feedback: ** \\\n",
    "          ** enhancement: ** \\\n",
    "          \\\n",
    "          **2. feedback: ** \\\n",
    "          ** enhancement: ** \\\n",
    "          \\\n",
    "          **3. feedback: ** \\\n",
    "          ** enhancement: ** \\\n",
    "          \\\n",
    "          **4. feedback: ** \\\n",
    "          ** enhancement: ** \\\n",
    "          ... \\\n",
    "          \\\n",
    "          ## Weekness Point: \\\n",
    "          ## How to training: \\\n",
    "          \"}\n",
    "\n",
    "user3 = { \"role\": \"user\", \"content\": f\"Compare the following expert and user pose data for approximately between 10 and 15 second of 150 frames and provide feedback on the user's performance. Focus on major differences and suggest improvements. {formatted_data3}\"}\n",
    "\n",
    "response3 = model.generate_content(f\"[{system3}, {user3}]\")\n",
    "\n",
    "print(response3.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 451~ Frame Poses Compare\n",
    "- [Expert Images](expert/frame_451.jpg)\n",
    "- [User Images](user/frame_451.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Feedback: Pose data anlysis(15s~)\n",
      "**1. feedback: ** The user's right shoulder is lower than the expert's. This is particularly noticeable around frame 130.\n",
      "** enhancement: ** The user should try to keep their right shoulder at a similar height to their left shoulder. This will help to create a more balanced and symmetrical pose.\n",
      "**2. feedback: ** The user's left hip is slightly higher than the expert's.\n",
      "** enhancement: ** The user should try to keep their hips at a similar height. This will help to create a more stable and grounded base.\n",
      "**3. feedback: ** The user's left knee is less bent than the expert's, which makes the user's pose less dynamic.\n",
      "** enhancement: ** The user should try to bend their left knee more deeply, as this will create a more powerful and dynamic pose.\n",
      "**4. feedback: ** The user's left foot is not pointing as far out as the expert's, indicating a lack of engagement in the left leg.\n",
      "** enhancement: ** The user should try to extend their left foot out further, as this will help to create a more fluid and graceful transition between poses.\n",
      "\n",
      "## Weekness Point: \n",
      "The user's pose lacks dynamism and balance, and their left leg is not fully engaged. \n",
      "\n",
      "## How to training:\n",
      "The user should focus on achieving a more balanced and symmetrical pose, with equal engagement in both legs. They should practice bending their left knee more deeply, extending their left foot further, and maintaining a stable and grounded base. The user can also practice mirroring the expert's pose frame by frame to better understand the desired form and movement. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "def format_pose_data4(expert_poses, user_poses):\n",
    "    # 포즈 데이터를 GPT에 전송할 수 있는 형식으로 변환\n",
    "    formatted_data = {\n",
    "        \"expert_poses\": [\n",
    "            [{\"x\": lm.x, \"y\": lm.y, \"z\": lm.z} for lm in pose]\n",
    "            for pose in expert_poses[450:]  # 처음 90개 프레임만 사용\n",
    "        ],\n",
    "        \"user_poses\": [\n",
    "            [{\"x\": lm.x, \"y\": lm.y, \"z\": lm.z} for lm in pose]\n",
    "            for pose in user_poses[450:]  # 처음 90개 프레임만 사용\n",
    "        ]\n",
    "    }\n",
    "    return json.dumps(formatted_data)\n",
    "\n",
    "formatted_data4 = format_pose_data4(expert_poses, user_poses)\n",
    "\n",
    "import google.generativeai as genai\n",
    "\n",
    "genai.configure(api_key=os.getenv('GEMINI_API_KEY'))\n",
    "\n",
    "model = genai.GenerativeModel('gemini-1.5-flash')\n",
    "\n",
    "system4 = { \"role\": \"system\", \"content\": f\"You are a dance instructor analyzing pose data. \\\n",
    "           Response Form: markdown \\\n",
    "           # Feedback: Pose data anlysis(15s~)\\\n",
    "           \\\n",
    "           **1. feedback: ** \\\n",
    "           ** enhancement: ** \\\n",
    "           \\\n",
    "           **2. feedback: ** \\\n",
    "           ** enhancement: ** \\\n",
    "           \\\n",
    "           **3. feedback: ** \\\n",
    "           ** enhancement: ** \\\n",
    "           \\\n",
    "           **4. feedback: ** \\\n",
    "           ** enhancement: ** \\\n",
    "           ... \\\n",
    "           \\\n",
    "           ## Weekness Point: \\\n",
    "           ## How to training: \\\n",
    "           \"}\n",
    "\n",
    "user4 = { \"role\": \"user\", \"content\": f\"Compare the following expert and user pose data for approximately after 15 second of 150 frames and provide feedback on the user's performance. Focus on major differences and suggest improvements. {formatted_data4}\"}\n",
    "\n",
    "response4 = model.generate_content(f\"[{system4}, {user4}]\")\n",
    "\n",
    "print(response4.text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
